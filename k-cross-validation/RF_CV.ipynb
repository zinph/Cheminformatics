{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following function splits the data into k different folds. It requires four parameters (in the form of lists, numpy array or pandas series) since it is simply partitioning the data into k-folds:\n",
    "\n",
    "a. DATA: features of the dataset. In this case, it would be molecular descriptors.\n",
    "b. TARGETS: biological endpoints of the dataset. In this case, it would be pKi.\n",
    "c. IDS: Identifiers of the entries. In this case, it would be ChEMBL IDs.\n",
    "d. folds: The number of folds you would want for your machine learning process. \n",
    "\n",
    "There will be two outputs: one-folds and nine-folds. The former "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_partitions(DATA,TARGETS,IDS, folds):\n",
    "    num_val_samples = len(DATA) // folds+1\n",
    "    one_fold = []\n",
    "    nine_folds = []\n",
    "    for i in range(folds):\n",
    "        one_fold_data = DATA[i * num_val_samples: (i + 1) * num_val_samples] # prepares the validation data: data from partition # k\n",
    "        one_fold_targets = TARGETS[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        one_fold_IDs = IDS[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        one_fold += [[one_fold_data, one_fold_targets, one_fold_IDs]]\n",
    "        \n",
    "        # prepares the training data: data from all other partitions\n",
    "        nine_fold_data = np.concatenate([DATA[:i * num_val_samples],DATA[(i + 1) * num_val_samples:]],axis=0)\n",
    "        nine_fold_targets = np.concatenate([TARGETS[:i * num_val_samples],TARGETS[(i + 1) * num_val_samples:]],axis=0)\n",
    "        nine_fold_IDs = np.concatenate([IDS[:i * num_val_samples],IDS[(i + 1) * num_val_samples:]],axis=0)\n",
    "        nine_folds += [[nine_fold_data,nine_fold_targets,nine_fold_IDs]]\n",
    "    return one_fold, nine_folds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and shuffle \n",
    "\n",
    "directory = \"Data/\"\n",
    "df = pd.read_csv(directory+\"sample_dataset.csv\", sep=',')\n",
    "df = shuffle(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_all_data    = df.loc[:, df.columns != 'Aff'].drop(df.columns[0],axis=1)\n",
    "\n",
    "print(pre_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data, normalize columns\n",
    "imputer      = SimpleImputer()\n",
    "scaler       = preprocessing.MinMaxScaler()\n",
    "all_data     = scaler.fit_transform(imputer.fit_transform(pre_all_data)) \n",
    "\n",
    "print(\"all_data :\", all_data.shape, \"all_data_type: \", type(all_data))\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_k = 10\n",
    "\n",
    "test_fold,train_fold = split_partitions(all_data,df['Aff'],df['ID'],outer_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outerCV__targets = []\n",
    "outerCV__predictions = []\n",
    "outerCV__IDs = [] \n",
    "\n",
    "cv_frame = pd.DataFrame()\n",
    "\n",
    "for i in range(outer_k):\n",
    "\n",
    "    outer_train = train_fold[i]\n",
    "    outer_test  = test_fold[i]\n",
    "\n",
    "    outerCV__test_data, outerCV__test_targets, outerCV__test_ids  = test_fold[i][0], test_fold[i][1], test_fold[i][2] \n",
    "    outerCV__train_data, outerCV__train_targets, outerCV__train_ids = train_fold[i][0], train_fold[i][1], train_fold[i][2]\n",
    "\n",
    "\n",
    "    cv_rf = RandomForestRegressor(n_estimators= 1600, max_depth = 90, \n",
    "                           max_features = 'auto', min_samples_leaf = 1, \n",
    "                           min_samples_split = 5, bootstrap = True, criterion=\"mae\", n_jobs = 10)\n",
    "    # Fit the random search model\n",
    "    cv_rf.fit(outerCV__train_data,outerCV__train_targets)\n",
    "\n",
    "    outerCV__test_predictions = cv_rf.predict(outerCV__test_data).tolist()\n",
    "    outerCV__predictions.append(outerCV__test_predictions)\n",
    "    outerCV__targets.append(outerCV__test_targets)\n",
    "    outerCV__IDs.append(outerCV__test_ids)\n",
    "    del cv_rf, outerCV__test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outerCV__targets_combined  = list(itertools.chain.from_iterable(outerCV__targets))\n",
    "outerCV__predictions_combined = list(itertools.chain.from_iterable(outerCV__predictions))\n",
    "outerCV__IDs_combined = list(itertools.chain.from_iterable(outerCV__IDs))\n",
    "\n",
    "cv_frame['IDs'] = outerCV__IDs_combined\n",
    "cv_frame['ExperimentalAff'] = outerCV__targets_combined\n",
    "cv_frame['PredictedAff'] = outerCV__predictions_combined\n",
    "\n",
    "cv_frame.to_csv(directory+'RF_CV_BestModel_Predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = mean_squared_error(outerCV__targets_combined, outerCV__predictions_combined, squared=False)\n",
    "print(\"rms error is: \" + str(rms))\n",
    "\n",
    "\n",
    "r2 = r2_score(outerCV__targets_combined, outerCV__predictions_combined)\n",
    "print(\"r2 value is: \" + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
