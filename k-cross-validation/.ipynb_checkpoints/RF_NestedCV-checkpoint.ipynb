{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The following function splits the data into k different folds. It requires four parameters (in the form of lists, numpy array or pandas series) since it is simply partitioning the data into k-folds:\n",
    "\n",
    "a. DATA: features of the dataset. In this case, it would be molecular descriptors.\n",
    "b. TARGETS: biological endpoints of the dataset. In this case, it would be pKi.\n",
    "c. IDS: Identifiers of the entries. In this case, it would be ChEMBL IDs.\n",
    "d. folds: The number of folds you would want for your machine learning process. \n",
    "\n",
    "There will be two outputs: one folds (test sets) and k-1 folds (train sets). \n",
    "They are heavily nested. \n",
    "The former contains a list of test sets and the later train sets. If you chose 5 fold, it will contain 5 test sets and 5 train sets. Each set is a list containing 3 lists in this order: processed data, endpoint, Ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_partitions(DATA,TARGETS,IDS, folds):\n",
    "    num_val_samples = len(DATA) // folds+1\n",
    "    one_fold = []\n",
    "    nine_folds = []\n",
    "    for i in range(folds):\n",
    "        one_fold_data = DATA[i * num_val_samples: (i + 1) * num_val_samples] # prepares the validation data: data from partition # k\n",
    "        one_fold_targets = TARGETS[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        one_fold_IDs = IDS[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        one_fold += [[one_fold_data, one_fold_targets, one_fold_IDs]]\n",
    "        \n",
    "        # prepares the training data: data from all other partitions\n",
    "        nine_fold_data = np.concatenate([DATA[:i * num_val_samples],DATA[(i + 1) * num_val_samples:]],axis=0)\n",
    "        nine_fold_targets = np.concatenate([TARGETS[:i * num_val_samples],TARGETS[(i + 1) * num_val_samples:]],axis=0)\n",
    "        nine_fold_IDs = np.concatenate([IDS[:i * num_val_samples],IDS[(i + 1) * num_val_samples:]],axis=0)\n",
    "        nine_folds += [[nine_fold_data,nine_fold_targets,nine_fold_IDs]]\n",
    "    return one_fold, nine_folds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_BestParams(data, targets):\n",
    "    '''\n",
    "    Based on data and targets, search for the best parameters and return them\n",
    "    '''\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "    \n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    \n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 60, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    \n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    \n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    \n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    \n",
    "    rf = RandomForestRegressor(criterion=\"mse\")\n",
    "    \n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                                       n_iter = 5, cv = 5, verbose=2, \n",
    "                                       random_state=42, n_jobs = 2)\n",
    "    rf_random.fit(data, targets)\n",
    "    best_params = rf_random.best_params_\n",
    "    \n",
    "    print(best_params)\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID  Smax38  Smax39   Geto  Chi5ch  Smax35  Smax36  Smax37  \\\n",
      "468   CHEMBL402288       0       0  2.064   0.118   0.000     0.0  13.141   \n",
      "57   CHEMBL3689642       0       0  2.130   0.000   0.000     0.0   0.000   \n",
      "479   CHEMBL406845       0       0  2.006   0.079   0.000     0.0   0.000   \n",
      "74   CHEMBL3689651       0       0  2.165   0.000   0.000     0.0  13.956   \n",
      "312  CHEMBL3685141       0       0  2.074   0.000   5.713     0.0   0.000   \n",
      "..             ...     ...     ...    ...     ...     ...     ...     ...   \n",
      "358  CHEMBL3685064       0       0  2.135   0.000   0.000     0.0   0.000   \n",
      "184   CHEMBL255165       0       0  2.037   0.118   0.000     0.0  12.879   \n",
      "327  CHEMBL3685159       0       0  2.032   0.000   5.746     0.0   0.000   \n",
      "296  CHEMBL3685016       0       0  2.135   0.000   0.000     0.0  13.282   \n",
      "117  CHEMBL3685124       0       0  2.124   0.000   5.025     0.0  13.575   \n",
      "\n",
      "     Smax30  Smax31  ...   MZM1  VSAEstate8  nrot    Sito  Smax46  Chi10  \\\n",
      "468   0.000   0.000  ...  7.278      37.935     5   7.554       0  0.734   \n",
      "57    0.000   0.000  ...  7.250      48.457     4   8.207       0  1.295   \n",
      "479   0.000   1.879  ...  7.840      38.530     1   6.350       0  0.801   \n",
      "74    0.000   0.000  ...  8.222      52.378     4  10.065       0  2.097   \n",
      "312   0.000   0.000  ...  6.528      45.500     5   6.651       0  0.830   \n",
      "..      ...     ...  ...    ...         ...   ...     ...     ...    ...   \n",
      "358   0.000   1.857  ...  7.000      53.643     3   7.906       0  1.223   \n",
      "184  -0.447   0.000  ...  8.389      56.184     6   8.031       0  0.805   \n",
      "327   0.000   0.000  ...  7.479      51.417     5   6.776       0  0.812   \n",
      "296   0.000   0.000  ...  7.000      42.885     3   7.906       0  1.207   \n",
      "117   0.000   0.000  ...  7.500      48.592     3   8.509       0  1.306   \n",
      "\n",
      "     Sfinger20  VSAEstate9  nheavy   Aff  \n",
      "468          0       8.680      24  6.57  \n",
      "57           0       6.320      25  9.52  \n",
      "479          0       5.951      21  6.30  \n",
      "74           0       0.000      30  8.52  \n",
      "312          0       0.000      21  7.92  \n",
      "..         ...         ...     ...   ...  \n",
      "358          0       0.000      24  7.38  \n",
      "184          0       2.620      26  6.68  \n",
      "327          0       0.000      22  7.96  \n",
      "296          0       0.000      24  9.52  \n",
      "117          0       0.000      26  9.70  \n",
      "\n",
      "[555 rows x 649 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and shuffle \n",
    "\n",
    "directory = \"Data/\"\n",
    "df = pd.read_csv(directory+\"sample_dataset.csv\", sep=',')\n",
    "df = shuffle(df, random_state=65)\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Smax38  Smax39   Geto  Chi5ch  Smax35  Smax36  Smax37  Smax30  Smax31  \\\n",
      "468       0       0  2.064   0.118   0.000     0.0  13.141   0.000   0.000   \n",
      "57        0       0  2.130   0.000   0.000     0.0   0.000   0.000   0.000   \n",
      "479       0       0  2.006   0.079   0.000     0.0   0.000   0.000   1.879   \n",
      "74        0       0  2.165   0.000   0.000     0.0  13.956   0.000   0.000   \n",
      "312       0       0  2.074   0.000   5.713     0.0   0.000   0.000   0.000   \n",
      "..      ...     ...    ...     ...     ...     ...     ...     ...     ...   \n",
      "358       0       0  2.135   0.000   0.000     0.0   0.000   0.000   1.857   \n",
      "184       0       0  2.037   0.118   0.000     0.0  12.879  -0.447   0.000   \n",
      "327       0       0  2.032   0.000   5.746     0.0   0.000   0.000   0.000   \n",
      "296       0       0  2.135   0.000   0.000     0.0  13.282   0.000   0.000   \n",
      "117       0       0  2.124   0.000   5.025     0.0  13.575   0.000   0.000   \n",
      "\n",
      "     Smax32  ...   MZM2   MZM1  VSAEstate8  nrot    Sito  Smax46  Chi10  \\\n",
      "468       0  ...  1.184  7.278      37.935     5   7.554       0  0.734   \n",
      "57        0  ...  1.263  7.250      48.457     4   8.207       0  1.295   \n",
      "479       0  ...  0.951  7.840      38.530     1   6.350       0  0.801   \n",
      "74        0  ...  1.345  8.222      52.378     4  10.065       0  2.097   \n",
      "312       0  ...  0.997  6.528      45.500     5   6.651       0  0.830   \n",
      "..      ...  ...    ...    ...         ...   ...     ...     ...    ...   \n",
      "358       0  ...  1.077  7.000      53.643     3   7.906       0  1.223   \n",
      "184       0  ...  1.289  8.389      56.184     6   8.031       0  0.805   \n",
      "327       0  ...  1.091  7.479      51.417     5   6.776       0  0.812   \n",
      "296       0  ...  1.077  7.000      42.885     3   7.906       0  1.207   \n",
      "117       0  ...  1.183  7.500      48.592     3   8.509       0  1.306   \n",
      "\n",
      "     Sfinger20  VSAEstate9  nheavy  \n",
      "468          0       8.680      24  \n",
      "57           0       6.320      25  \n",
      "479          0       5.951      21  \n",
      "74           0       0.000      30  \n",
      "312          0       0.000      21  \n",
      "..         ...         ...     ...  \n",
      "358          0       0.000      24  \n",
      "184          0       2.620      26  \n",
      "327          0       0.000      22  \n",
      "296          0       0.000      24  \n",
      "117          0       0.000      26  \n",
      "\n",
      "[555 rows x 647 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop ID and Aff columns. Keep the dataframe containing only the molecular descriptors.\n",
    "\n",
    "pre_all_data    = df.drop(['ID', 'Aff'], axis=1)\n",
    "print(pre_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data : (555, 647) all_data_type:  <class 'numpy.ndarray'>\n",
      "[[0.         0.         0.34615385 ... 0.         0.54804506 0.25925926]\n",
      " [0.         0.         0.57692308 ... 0.         0.44378175 0.2962963 ]\n",
      " [0.         0.         0.14335664 ... 0.         0.42747957 0.14814815]\n",
      " ...\n",
      " [0.         0.         0.23426573 ... 0.         0.16456815 0.18518519]\n",
      " [0.         0.         0.59440559 ... 0.         0.16456815 0.25925926]\n",
      " [0.         0.         0.55594406 ... 0.         0.16456815 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "# preprocess data, normalize columns\n",
    "\n",
    "imputer      = SimpleImputer()\n",
    "imputed_data = imputer.fit_transform(pre_all_data)\n",
    "\n",
    "scaler       = preprocessing.MinMaxScaler()\n",
    "all_data     = scaler.fit_transform(imputed_data) \n",
    "\n",
    "print(\"all_data :\", all_data.shape, \"all_data_type: \", type(all_data))\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_k = 5\n",
    "\n",
    "test_fold, train_fold = split_partitions(all_data,df['Aff'],df['ID'],outer_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform k-fold CV with RF algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:   19.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 48, 'bootstrap': True}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:   23.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 48, 'bootstrap': True}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:   19.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 48, 'bootstrap': True}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:   18.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 43, 'bootstrap': False}\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  25 out of  25 | elapsed:   22.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 48, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "outerCV__targets = []\n",
    "outerCV__predictions = []\n",
    "outerCV__IDs = [] \n",
    "\n",
    "cv_frame = pd.DataFrame()\n",
    "\n",
    "for i in range(outer_k):\n",
    "\n",
    "    outer_train = train_fold[i]\n",
    "    outer_test  = test_fold[i]\n",
    "\n",
    "    outerCV__test_data, outerCV__test_targets, outerCV__test_ids  = test_fold[i][0], test_fold[i][1], test_fold[i][2] \n",
    "    outerCV__train_data, outerCV__train_targets, outerCV__train_ids = train_fold[i][0], train_fold[i][1], train_fold[i][2]\n",
    "\n",
    "    ### RF ###\n",
    "    \n",
    "    rf_best_params = RF_BestParams(outerCV__train_data, outerCV__train_targets)\n",
    "\n",
    "    rf_best = None\n",
    "    rf_best = RandomForestRegressor(n_estimators = rf_best_params['n_estimators'],\n",
    "                                    max_features = rf_best_params['max_features'],\n",
    "                                    max_depth = rf_best_params['max_depth'],\n",
    "                                    min_samples_split = rf_best_params['min_samples_split'],\n",
    "                                    min_samples_leaf = rf_best_params['min_samples_leaf'],\n",
    "                                    bootstrap = rf_best_params['bootstrap'])\n",
    "\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_best.fit(outerCV__train_data,outerCV__train_targets)\n",
    "\n",
    "    outerCV__test_predictions = rf_best.predict(outerCV__test_data).tolist()\n",
    "    \n",
    "    outerCV__predictions.append(outerCV__test_predictions)\n",
    "    outerCV__targets.append(outerCV__test_targets)\n",
    "    outerCV__IDs.append(outerCV__test_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outerCV__targets_combined  = list(itertools.chain.from_iterable(outerCV__targets))\n",
    "outerCV__predictions_combined = list(itertools.chain.from_iterable(outerCV__predictions))\n",
    "outerCV__IDs_combined = list(itertools.chain.from_iterable(outerCV__IDs))\n",
    "\n",
    "cv_frame['IDs'] = outerCV__IDs_combined\n",
    "cv_frame['ExperimentalAff'] = outerCV__targets_combined\n",
    "cv_frame['PredictedAff'] = outerCV__predictions_combined\n",
    "\n",
    "cv_frame.to_csv(directory+'RF_CV_BestModel_Predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = mean_squared_error(outerCV__targets_combined, outerCV__predictions_combined, squared=False)\n",
    "print(\"rms error is: \" + str(rms))\n",
    "\n",
    "\n",
    "r2 = r2_score(outerCV__targets_combined, outerCV__predictions_combined)\n",
    "print(\"r2 value is: \" + str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms error is: 0.6831274867421843\n",
    "r2 value is: 0.7546659855002342"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
