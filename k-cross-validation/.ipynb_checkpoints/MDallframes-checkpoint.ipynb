{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from keras import backend as K\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth=True\n",
    "#sess = tf.Session(config=config)\n",
    "#K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.decomposition import PCA\n",
    "from keras import regularizers\n",
    "#model_v2.h5 \n",
    "#765510\n",
    "#804536\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(200, kernel_regularizer=regularizers.l2(0.001), activation ='relu', input_shape=(765510,)))\n",
    "    #model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001),activation ='relu', input_shape=(self.nodes,)))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation ='relu'))\n",
    "    model.add(layers.Dense(80, kernel_regularizer=regularizers.l2(0.001), activation ='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from random import shuffle\n",
    "\n",
    "class MDframes_DL:\n",
    "    def __init__(self):\n",
    "        self.retrieve_data_labels()\n",
    "        self.retrieve_cpd_filenames()\n",
    "        \n",
    "    def retrieve_data_labels(self):\n",
    "        '''Retrieve the names of compounds along with binding affinities and save them in dictionary.'''\n",
    "        cpd_affmanager = open(\"/data/zinph/DeepLearning/aff.csv\",'r')\n",
    "        cpd_affmanager.readline()\n",
    "        self.cpd_data_labels = {i[0]:float(i[1]) for i in (x.strip().split(',') for x in cpd_affmanager)} # make dictionary of compound IDs (string) matched with binding affinities (float)\n",
    "        \n",
    "    def retrieve_cpd_filenames(self):\n",
    "        '''Retrieve the names of all cpds files in the folder and shuffle the list.'''\n",
    "        directory = \"/data/zinph/DeepLearning/Featured_MDallframes/\"\n",
    "        os.chdir(directory)\n",
    "        self.cpd_filenames = os.listdir(directory) # contains the names of all compounds in the folder\n",
    "        shuffle(self.cpd_filenames)\n",
    "        \n",
    "    def BatchGenerator(self,file_names):\n",
    "        os.chdir(\"/data/zinph/DeepLearning/Featured_MDallframes/\")\n",
    "        \"Based on files in path and generate data and labels batch by batch.\"\n",
    "        initial = 0\n",
    "        batch_size = 100\n",
    "        #if initial == len(file_names):\n",
    "        #    initial = 0\n",
    "        while initial < len(file_names):\n",
    "            pre_data = []\n",
    "            labels = []\n",
    "            for i in range(batch_size):\n",
    "                if initial < len(file_names):\n",
    "                    if os.path.isfile(file_names[initial]): \n",
    "                        #print(initial)\n",
    "                        each_cpd = pd.read_csv(file_names[initial])\n",
    "                        #each_cpd = each_cpd.where(pd.notna(each_cpd), each_cpd.mean(), axis='columns')\n",
    "                        df = each_cpd.fillna(0)\n",
    "                        #df = df.loc[:, (df != 0).any(axis=0)]\n",
    "                        #df = df.dropna(axis='columns')\n",
    "                        scaler = MinMaxScaler()\n",
    "                        each_cpd_scaled = scaler.fit_transform(df)\n",
    "                        pre_data.append(each_cpd_scaled)\n",
    "                        print(np.array(each_cpd_scaled).shape)\n",
    "                        labels.append(self.cpd_data_labels[file_names[initial][:-4]])\n",
    "                        initial += 1\n",
    "            pre_data = np.array(pre_data)\n",
    "            shaped_data = pre_data.reshape((pre_data.shape[0], pre_data.shape[1] * pre_data.shape[2]))\n",
    "            #nodes = shaped_data.shape[1]\n",
    "            #impute_data = Imputer(missing_values='NaN', strategy='mean', axis=0).fit_transform(shaped_data)\n",
    "            #print(shaped_data)\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            final_data = scaler.fit_transform(shaped_data)\n",
    "            yield final_data, labels\n",
    "      \n",
    "    def read_data(self, files):\n",
    "        pre_data = []\n",
    "        labels = []\n",
    "        for i in files:\n",
    "            each_cpd = pd.read_csv(i)\n",
    "            df = each_cpd.fillna(0)\n",
    "            scaler = MinMaxScaler()\n",
    "            each_cpd_scaled = scaler.fit_transform(df)\n",
    "            pre_data.append(each_cpd_scaled)\n",
    "            labels.append(self.cpd_data_labels[i[:-4]])\n",
    "        pre_data = np.nan_to_num(np.array(pre_data))\n",
    "        shaped_data = pre_data.reshape((pre_data.shape[0], pre_data.shape[1] * pre_data.shape[2]))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data = scaler.fit_transform(shaped_data)\n",
    "        return data, labels\n",
    "\n",
    "    def split_train_test(self):\n",
    "        '''Split the list of compound names. train_data has the first 700 rows, and the test the rest. Returns the list of train cpd names and test cpd names.'''\n",
    "        train_data_file_names = self.cpd_filenames[:700]\n",
    "        test_data_file_names = self.cpd_filenames[700:]\n",
    "        return train_data_file_names, test_data_file_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = MDframes_DL()\n",
    "\n",
    "train_data_files, test_data_files = sample.split_train_test() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n"
     ]
    }
   ],
   "source": [
    "# k-fold validation\n",
    "import numpy as np\n",
    "k = 4\n",
    "num_val_samples = len(train_data_files) // k\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data_files = train_data_files[i * num_val_samples: (i + 1) * num_val_samples]  # prepares the validation data from partition #k\n",
    "    val_data, val_targets = sample.read_data(val_data_files)   \n",
    "    \n",
    "    partial_train_data_files = np.concatenate(  # prepares the training data from all other partitions\n",
    "        [train_data_files[:i * num_val_samples],\n",
    "        train_data_files[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    partial_train_data, partial_train_targets = sample.read_data(partial_train_data_files)\n",
    "    \n",
    "    model = build_model()  # builds the Keras model (already compiled)\n",
    "    history = model.fit(partial_train_data, np.array(partial_train_targets), # trains the model (in silent mode, verbose=0)\n",
    "                       validation_data = (val_data, np.array(val_targets)),\n",
    "                       epochs = num_epochs, batch_size=1, verbose=0)\n",
    "    \n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1,len(average_mae_history)+1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.rcParams[\"figure.dpi\"] = 1000\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "#print(train_data)\n",
    "import time\n",
    "start = time.time()\n",
    "#all_mae_histories = []\n",
    "\n",
    "for e in range(260):\n",
    "    x = sample.BatchGenerator(train_data_files)\n",
    "    #print(\"epoch %d\" % e)\n",
    "    #train_data_set = train_data*100\n",
    "    for train_data,train_targets in x:\n",
    "        history = model.fit(train_data, np.array(train_targets)) \n",
    "        \n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/data/zinph/DeepLearning/MDallframes_script/model_reducedf_LG_BS_MDFPIallframes_v0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data_list = []\n",
    "#for test_data, test_targets in sample.BatchGenerator(test_data_files):\n",
    "#    test_data_list.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_mae = [],[]\n",
    "x = sample.BatchGenerator(test_data_files)\n",
    "#print(test_data_files, len(test_data_files))\n",
    "for test_data,test_targets in x:\n",
    "    #print(test_targets)\n",
    "    test_mse_score, test_mae_score = model.evaluate(test_data, np.array(test_targets))\n",
    "    test_mse.append(test_mse_score)\n",
    "    test_mae.append(test_mae_score)\n",
    "#test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_test_mse_score = np.mean(test_mse)\n",
    "average_test_mae_score = np.mean(test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE score: \", average_test_mae_score, ' , ', \"MSE score: \", average_test_mse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data_files, len(test_data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predictions = []\n",
    "targets = []\n",
    "for test_data,test_targets in sample.BatchGenerator(test_data_files):\n",
    "    #print(test_data)\n",
    "    predictions.append(model.predict(test_data))\n",
    "    targets.append(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "#flattened = np.concatenate(predictions,axis=0)\n",
    "test_predictions = np.concatenate(predictions, axis=0)\n",
    "test_targets = reduce(operator.add, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyfit(test_targets, test_predictions, deg=1)\n",
    "plt.plot(test_targets, fit[0] * test_targets + fit[1], color='red')\n",
    "plt.scatter(test_targets, test_predictions,s=10)\n",
    "plt.xlabel('Experimental pKi')\n",
    "plt.ylabel('Predicted pKi')\n",
    "plt.rcParams[\"figure.dpi\"] = 1000\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [i for i in test_predictions.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions\n",
    "flat_list = [item for sublist in test_predictions for item in sublist]\n",
    "test_predictions = flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(test_targets, test_predictions)\n",
    "print('slope, intercept, r_value, p_value, std_err =',slope,',', intercept,',', r_value,',', p_value,',', std_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_files = train_data_files + test_data_files\n",
    "for data,targets in sample.BatchGenerator(all_files):\n",
    "    #print(model.predict(test_data))\n",
    "    all_predictions.append(model.predict(data))\n",
    "    all_targets.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "all_targets = reduce(operator.add, all_targets)\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = np.polyfit(all_targets, all_predictions, deg=1)\n",
    "plt.plot(all_targets, fit[0] * all_targets + fit[1], color='red')\n",
    "plt.scatter(all_targets, all_predictions)\n",
    "plt.xlabel('Experimental Affinity')\n",
    "plt.ylabel('Predicted Affinity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "flat_alllist = [item for sublist in all_predictions for item in sublist]\n",
    "all_final_predictions = flat_alllist\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(all_targets, all_final_predictions)\n",
    "print('slope, intercept, r_value, p_value, std_err =',slope,',', intercept,',', r_value,',', p_value,',', std_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
